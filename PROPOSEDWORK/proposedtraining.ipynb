{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10975614,"sourceType":"datasetVersion","datasetId":6829647},{"sourceId":10982383,"sourceType":"datasetVersion","datasetId":6834830},{"sourceId":11091289,"sourceType":"datasetVersion","datasetId":6913778},{"sourceId":11196918,"sourceType":"datasetVersion","datasetId":6990574},{"sourceId":11281611,"sourceType":"datasetVersion","datasetId":7053341},{"sourceId":11272613,"sourceType":"datasetVersion","datasetId":7046797}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:03:36.821429Z","iopub.execute_input":"2025-04-05T09:03:36.821705Z","iopub.status.idle":"2025-04-05T09:03:37.132542Z","shell.execute_reply.started":"2025-04-05T09:03:36.821684Z","shell.execute_reply":"2025-04-05T09:03:37.131835Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sentencesfold/sentences.csv\n/kaggle/input/proposedworkk/proposedworktrainingdata.csv\n/kaggle/input/3000dataset/sentences2.csv\n/kaggle/input/3000dataset/proposedworktrainingdata.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================\n# 1. Install Dependencies\n# =========================\n# If you're in a fresh environment (e.g., a Kaggle notebook), install:\n!pip install torch==2.0.1  # or a compatible PyTorch version\n!pip install transformers==4.30.2  # or a compatible Transformers version\n!pip install datasets==2.12.0\n!pip install peft==0.3.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:03:40.091610Z","iopub.execute_input":"2025-04-05T09:03:40.092114Z","iopub.status.idle":"2025-04-05T09:05:52.968727Z","shell.execute_reply.started":"2025-04-05T09:03:40.092079Z","shell.execute_reply":"2025-04-05T09:05:52.967824Z"}},"outputs":[{"name":"stdout","text":"Collecting torch==2.0.1\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.17.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.0.0 (from torch==2.0.1)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.2)\nCollecting lit (from triton==2.0.0->torch==2.0.1)\n  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\nDownloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.5.0.post0 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\nCollecting transformers==4.30.2\n  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (2.32.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.2) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.30.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.30.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.30.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.30.2) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.30.2) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.30.2) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.2) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.30.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.30.2) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.30.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.30.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers==4.30.2) (2024.2.0)\nDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.30.2\nCollecting datasets==2.12.0\n  Downloading datasets-2.12.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (1.26.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (19.0.1)\nCollecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0)\n  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (0.70.16)\nRequirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.12.0) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (3.11.12)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (24.2)\nCollecting responses<0.19 (from datasets==2.12.0)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (1.18.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.12.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.12.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.12.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.12.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.12.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.12.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.12.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.12.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.12.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.12.0) (2025.1.31)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.12.0)\n  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.12.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.12.0) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.12.0) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.12.0) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==2.12.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==2.12.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==2.12.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets==2.12.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets==2.12.0) (2024.2.0)\nDownloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dill, responses, multiprocess, datasets\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.3.1\n    Uninstalling datasets-3.3.1:\n      Successfully uninstalled datasets-3.3.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.14 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.12.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0\nCollecting peft==0.3.0\n  Downloading peft-0.3.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (2.0.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (4.30.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.3.0) (1.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.3.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.3.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.3.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.3.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.3.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.3.0) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (3.17.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.3.0) (2.0.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0) (75.1.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.3.0) (0.45.1)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0) (3.31.2)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0) (18.1.8)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->peft==0.3.0) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->peft==0.3.0) (0.4.5)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (2.32.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.3.0) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate->peft==0.3.0) (2024.12.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.3.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.3.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.3.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft==0.3.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft==0.3.0) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.3.0) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.3.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft==0.3.0) (2024.2.0)\nDownloading peft-0.3.0-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\nSuccessfully installed peft-0.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport gc\nimport random\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n)\nfrom peft import LoraConfig, get_peft_model\n\n# ---------------------------------------------------------------------\n# 1. Basic GPU Check\n# ---------------------------------------------------------------------\nprint(\"Number of GPUs available:\", torch.cuda.device_count())\nif torch.cuda.is_available():\n    print(\"GPU Name:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"No GPU detected\")\n\n# ---------------------------------------------------------------------\n# 2. Load Base Model & Tokenizer\n# ---------------------------------------------------------------------\nmodel_name = \"Orkhan/llama-2-7b-absa\"  # Replace with your model name\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    return_dict=True,\n    torch_dtype=torch.float16\n)\nbase_model.to(\"cuda:0\")\nbase_model.config.use_cache = False\nbase_model.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# ---------------------------------------------------------------------\n# 3. Load CSV (New Format) and Create 80/20 Train/Eval Split\n# ---------------------------------------------------------------------\n# The CSV is expected to have columns: text, span, opinion, sentiment\nimport pandas as pd\nfrom datasets import Dataset\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/3000dataset/proposedworktrainingdata.csv\")  # Update with your CSV path\nprint(\"Dataset columns:\", df.columns.tolist())\n\n# Apply case folding\ndf = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n\n# Manually split without shuffling\nsplit_index = int(0.8 * len(df))  # First 80% for training\ntrain_df = df.iloc[:split_index]  # First 80% rows\neval_df = df.iloc[split_index:]   # Last 20% rows\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\neval_dataset = Dataset.from_pandas(eval_df)\n\n# ---------------------------------------------------------------------\n# 4. Preprocessing Function\n# ---------------------------------------------------------------------\ndef preprocess_function(example):\n    # Build the prompt using all annotations from the CSV file.\n    target = (\n        f\"Aspect detected: {example['span']} ## \"\n        f\"Opinion detected: {example['opinion']} ## \"\n        f\"Sentiment detected: {example['sentiment']}\"\n    )\n    input_text = f\"### Human: {example['text']} ### Assistant: {target}\"\n    \n    tokenized = tokenizer(\n        input_text,\n        truncation=True,\n        max_length=256,  # Adjust as necessary\n        padding=\"max_length\"\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n# Map the preprocessing function over the datasets.\ntrain_dataset = train_dataset.map(preprocess_function, batched=False, remove_columns=train_dataset.column_names)\neval_dataset  = eval_dataset.map(preprocess_function, batched=False, remove_columns=eval_dataset.column_names)\n\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\neval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# ---------------------------------------------------------------------\n# 5. Apply LoRA\n# ---------------------------------------------------------------------\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n\n# ---------------------------------------------------------------------\n# 6. Training Arguments: 5 Epochs, Logging/Eval Once Per Epoch\n# ---------------------------------------------------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"results\",\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,  # Increased accumulation steps to 16\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    disable_tqdm=False,\n    log_level=\"error\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-4,\n    fp16=True,\n    report_to=\"none\"\n)\n\n\ndef custom_data_collator(features):\n    batch = {}\n    for key in features[0].keys():\n        collated = []\n        for f in features:\n            value = f[key]\n            if not torch.is_tensor(value):\n                value = torch.tensor(value)\n            if value.ndim == 0:\n                value = value.unsqueeze(0)\n            if value.ndim == 1:\n                value = value.unsqueeze(0)\n            collated.append(value)\n        batch[key] = torch.cat(collated, dim=0)\n    return batch\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,\n    data_collator=custom_data_collator,\n)\n\n# ---------------------------------------------------------------------\n# 7. Train the Model (5 Epochs)\n# ---------------------------------------------------------------------\nprint(\"Starting training for 5 epochs on a single GPU...\")\ntrainer.train()\nprint(\"Training complete.\")\n\n# ---------------------------------------------------------------------\n# 8. Merge LoRA + Base Weights, Save Full Model\n# ---------------------------------------------------------------------\nprint(\"Merging LoRA into base weights...\")\nmodel = model.merge_and_unload()\n\nsave_dir = \"finetuned_model\"\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\nprint(f\"✅ Full model saved to '{save_dir}' with config.json, etc.\")\n\n# ---------------------------------------------------------------------\n# 9. Free GPU Memory and Reload for Inference\n# ---------------------------------------------------------------------\ndel model, base_model, trainer\ntorch.cuda.empty_cache()\ngc.collect()\n\nfrom transformers import AutoModelForCausalLM\nprint(f\"Loading merged model from '{save_dir}'...\")\ninference_model = AutoModelForCausalLM.from_pretrained(save_dir, torch_dtype=torch.float16)\ninference_model.to(\"cuda:0\")\ninference_model.eval()\nprint(\"✅ Merged model loaded successfully! Ready for inference.\")\n\n# ---------------------------------------------------------------------\n# 10. Example Inference\n# ---------------------------------------------------------------------\nfrom transformers import pipeline\ndef process_prompt(user_prompt, model):\n    text_input = f\"### Human: {user_prompt} ###\"\n    pipe = pipeline(\n        task=\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_length=int(len(tokenizer.encode(user_prompt)) * 3.5),\n        device=0\n    )\n    return pipe(text_input)\n\ntest_sentence = \"The food is fresh at a good price, and the place is clean and hygienic.My stay at Hotel Tranquil Haven was pleasant thanks to its serene location and friendly staff.Overall, this hotel is suitable for a peaceful retreat, but overall experience could be way better.\"\nresult = process_prompt(test_sentence, inference_model)\nprint(\"\\nInference Result:\")\nprint(result[0][\"generated_text\"])\n\n##################################\nimport pandas as pd\nfrom transformers import pipeline\n\n# Define a function to process a prompt using your fine-tuned model.\ndef process_prompt(user_prompt, model):\n    # Construct the input with the expected prompt format.\n    text_input = f\"### Human: {user_prompt} ###\"\n    # Create a pipeline for text generation.\n    pipe = pipeline(\n        task=\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_length=int(len(tokenizer.encode(user_prompt)) * 3.5),\n        device=0\n    )\n    return pipe(text_input)\n\n# Path to the CSV file containing sentences (only a \"text\" column)\ninput_file = \"/kaggle/input/sentencesfold/sentences.csv\"  # Update with your file path\noutput_file = \"predicted_annotations.csv\"  # The file where predictions will be saved\n\n# Load the CSV file of sentences.\ndf = pd.read_csv(input_file)\n\n# Apply case folding (convert to lowercase)\ndf[\"text\"] = df[\"text\"].str.lower()\n\n# Prepare a list to store predictions.\npredictions = []\n\n# Iterate over each sentence in the CSV file.\nfor idx, row in df.iterrows():\n    sentence = row[\"text\"]\n    # Generate prediction using the fine-tuned model.\n    result = process_prompt(sentence, inference_model)\n    # Extract the generated text (assuming the output format is similar to your training example).\n    generated_text = result[0][\"generated_text\"]\n    predictions.append(generated_text)\n\n# Create a new DataFrame column for the predictions.\ndf[\"prediction\"] = predictions\n\n# Save the DataFrame with predictions to a new CSV file.\ndf.to_csv(output_file, index=False)\nprint(f\"Predictions saved to {output_file}\")\n\n##################################\n##################################\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, hamming_loss\n\n# -----------------------------\n# 1. Define functions to extract A-O-S triples\n# -----------------------------\n\ndef extract_aos_from_actual(row):\n    \"\"\"\n    Convert actual row into a set of A-O-S triples.\n    \"\"\"\n    aspects = [x.strip() for x in str(row['span']).split(\",\") if x.strip()]\n    opinions = [x.strip() for x in str(row['opinion']).split(\",\") if x.strip()]\n    sentiments = [x.strip() for x in str(row['sentiment']).split(\",\") if x.strip()]\n    aos_set = set(sorted(zip(aspects, opinions, sentiments)))\n    return aos_set\n\ndef extract_aos_from_pred(pred_str):\n    \"\"\"\n    Extract A-O-S triples from the predicted output.\n    \"\"\"\n    a_match = re.search(r\"aspect detected:\\s*(.*?)\\s*##\", pred_str)\n    o_match = re.search(r\"opinion detected:\\s*(.*?)\\s*##\", pred_str)\n    s_match = re.search(r\"sentiment detected:\\s*(.*)\", pred_str)\n\n    if a_match and o_match and s_match:\n        aspects = [x.strip() for x in a_match.group(1).split(\",\") if x.strip()]\n        opinions = [x.strip() for x in o_match.group(1).split(\",\") if x.strip()]\n        sentiments = [x.strip() for x in s_match.group(1).split(\",\") if x.strip()]\n        aos_set = set(sorted(zip(aspects, opinions, sentiments)))\n        return aos_set\n    else:\n        return set()\n\n# -----------------------------\n# 2. Load and preprocess actual and predicted data\n# -----------------------------\n\ndf_actual = pd.read_csv(\"/kaggle/input/proposedworkk/proposedworktrainingdata.csv\")\ndf_actual = df_actual.apply(lambda col: col.map(lambda x: x.lower().strip() if isinstance(x, str) else x))\ndf_actual['aos'] = df_actual.apply(extract_aos_from_actual, axis=1)\n\ndf_pred = pd.read_csv(\"/kaggle/working/predicted_annotations.csv\")\ndf_pred = df_pred.apply(lambda col: col.map(lambda x: x.lower().strip() if isinstance(x, str) else x))\ndf_pred['aos'] = df_pred['prediction'].apply(extract_aos_from_pred)\n\ndf_merged = df_actual[['text', 'aos']].merge(df_pred[['text', 'aos']], on='text', suffixes=('_actual', '_pred'))\n\n# -----------------------------\n# 3. Build a global universe of unique A-O-S triples\n# -----------------------------\n\nglobal_triples = sorted(set().union(*df_merged['aos_actual']).union(*df_merged['aos_pred']))\ntriple_to_idx = {triple: i for i, triple in enumerate(global_triples)}\n\ndef aos_to_vector(aos_set):\n    vec = [0] * len(global_triples)\n    for triple in aos_set:\n        if triple in triple_to_idx:\n            vec[triple_to_idx[triple]] = 1\n    return np.array(vec)\n\ndf_merged['vector_actual'] = df_merged['aos_actual'].apply(aos_to_vector)\ndf_merged['vector_pred'] = df_merged['aos_pred'].apply(aos_to_vector)\n\nactual_vectors = np.stack(df_merged['vector_actual'].values)\npred_vectors = np.stack(df_merged['vector_pred'].values)\n\n# -----------------------------\n# 4. Compute TP, TN, FP, FN and Metrics\n# -----------------------------\n\nTP = np.sum(np.logical_and(actual_vectors == 1, pred_vectors == 1))\nTN = np.sum(np.logical_and(actual_vectors == 0, pred_vectors == 0))\nFP = np.sum(np.logical_and(actual_vectors == 0, pred_vectors == 1))\nFN = np.sum(np.logical_and(actual_vectors == 1, pred_vectors == 0))\n\nprecision = TP / (TP + FP) if (TP + FP) > 0 else 0  # Precision = TP / (TP + FP)\nrecall = TP / (TP + FN) if (TP + FN) > 0 else 0  # Recall = TP / (TP + FN)\nf1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # F1 = 2 * (P * R) / (P + R)\nmcc = ((TP * TN) - (FP * FN)) / (np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))) if ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) > 0 else 0  # MCC formula\nhamming = hamming_loss(actual_vectors, pred_vectors)  # Hamming Loss formula: (FP + FN) / total samples\nfdr = FP / (FP + TP) if (FP + TP) > 0 else 0  # False Discovery Rate (FDR) = FP / (FP + TP)\n\nprint(\"\\n=== Evaluation Metrics ===\")\nprint(f\"True Positives (TP): {TP}\")\nprint(f\"True Negatives (TN): {TN}\")\nprint(f\"False Positives (FP): {FP}\")\nprint(f\"False Negatives (FN): {FN}\")\nprint(f\"Precision: {precision:.4f} (TP / (TP + FP))\")\nprint(f\"Recall: {recall:.4f} (TP / (TP + FN))\")\nprint(f\"F1 Score: {f1:.4f} (2 * (Precision * Recall) / (Precision + Recall))\")\nprint(f\"MCC: {mcc:.4f} (((TP * TN) - (FP * FN)) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN)))\")\nprint(f\"Hamming Loss: {hamming:.4f} ((FP + FN) / total samples)\")\nprint(f\"False Discovery Rate: {fdr:.4f} (FP / (FP + TP))\")\n\n# -----------------------------\n# 5. Check for Empty Predictions\n# -----------------------------\nempty_preds = df_merged[df_merged['aos_pred'].apply(lambda x: len(x) == 0)]\nif not empty_preds.empty:\n    print(\"\\n⚠️ Warning: Some predictions are empty!\")\n    print(empty_preds[['text', 'aos_actual', 'aos_pred']].head())\n# Save actual A-O-S triples\ndf_actual[['text', 'aos']].to_csv(\"/kaggle/working/actual_aos.csv\", index=False)\n\n# Save predicted A-O-S triples\ndf_pred[['text', 'aos']].to_csv(\"/kaggle/working/predicted_aos.csv\", index=False)\n\nprint(\"\\n✅ A-O-S files saved: 'actual_aos.csv' and 'predicted_aos.csv'\")\n\n\n##################################","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:06:59.845288Z","iopub.execute_input":"2025-04-05T09:06:59.845706Z","iopub.status.idle":"2025-04-05T15:02:00.051533Z","shell.execute_reply.started":"2025-04-05T09:06:59.845675Z","shell.execute_reply":"2025-04-05T15:02:00.050624Z"}},"outputs":[{"name":"stdout","text":"Number of GPUs available: 1\nGPU Name: Tesla P100-PCIE-16GB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a381db262f405689db02fcf9b06574"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2f2b58bcc8c40cc9598f0f1ec5f2271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da174d841c2e4ea9b435746f4e721aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d6b8453905b4147897446d02eae0d3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd12082440e441e83af9e7c0e39f9a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4c0cd14310498f98a2be5471fa0a6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9818fa50f44102a066c4ca5c384b8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0ad0617bc5a45eca4a514bc6714a8b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b7879b0a624dd5b7015fbb1fb1f912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6610dd51cc9341438b638f25de2eb266"}},"metadata":{}},{"name":"stdout","text":"Dataset columns: ['id', 'text', 'span', 'opinion', 'sentiment']\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-3-2f1cd2058b60>:53: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2564 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/641 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"trainable params: 8388608 || all params: 6746804224 || trainable%: 0.12433454005023165\nStarting training for 5 epochs on a single GPU...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12820' max='12820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12820/12820 4:24:48, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.262000</td>\n      <td>0.226798</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.194400</td>\n      <td>0.225772</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.143100</td>\n      <td>0.239426</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.097400</td>\n      <td>0.272965</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.067600</td>\n      <td>0.303588</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training complete.\nMerging LoRA into base weights...\n✅ Full model saved to 'finetuned_model' with config.json, etc.\nLoading merged model from 'finetuned_model'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce2d8447704b4e83a8faa0900f588e3e"}},"metadata":{}},{"name":"stdout","text":"✅ Merged model loaded successfully! Ready for inference.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nInference Result:\n### Human: The food is fresh at a good price, and the place is clean and hygienic.My stay at Hotel Tranquil Haven was pleasant thanks to its serene location and friendly staff.Overall, this hotel is suitable for a peaceful retreat, but overall experience could be way better. ### Assistant: Aspect detected: food, place, staff, hotel, experience ## Opinion detected: good, clean, friendly, suitable, better ## Sentiment detected: positive, positive, positive, neutral, negative\nPredictions saved to predicted_annotations.csv\n\n=== Evaluation Metrics ===\nTrue Positives (TP): 1578\nTrue Negatives (TN): 1393112\nFalse Positives (FP): 229\nFalse Negatives (FN): 396\nPrecision: 0.8733 (TP / (TP + FP))\nRecall: 0.7994 (TP / (TP + FN))\nF1 Score: 0.8347 (2 * (Precision * Recall) / (Precision + Recall))\nMCC: 0.8353 (((TP * TN) - (FP * FN)) / sqrt((TP+FP)(TP+FN)(TN+FP)(TN+FN)))\nHamming Loss: 0.0004 ((FP + FN) / total samples)\nFalse Discovery Rate: 0.1267 (FP / (FP + TP))\n\n⚠️ Warning: Some predictions are empty!\n                                               text  \\\n13  the food is always fresh and hot, ready to eat!   \n19                the menu is uneventful and small.   \n45                      the suggestion is absolute.   \n49           the staff is attentive and personable.   \n50         the staff is accommodating and friendly.   \n\n                                           aos_actual aos_pred  \n13  {(food, ready to eat, positive), (food, hot, p...       {}  \n19  {(menu, small, negative), (menu, uneventful, n...       {}  \n45                 {(suggestion, absolute, positive)}       {}  \n49  {(staff, attentive, positive), (staff, persona...       {}  \n50  {(staff, friendly, positive), (staff, accommod...       {}  \n\n✅ A-O-S files saved: 'actual_aos.csv' and 'predicted_aos.csv'\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink, display\n\nmodel_dir = \"finetuned_model\"\n\n# Iterate over each file in the directory\nfor filename in os.listdir(model_dir):\n    file_path = os.path.join(model_dir, filename)\n    # Create and display a download link for each file\n    display(FileLink(file_path, result_html_prefix=f\"👉 Download {filename}: \"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T15:17:40.559352Z","iopub.execute_input":"2025-04-05T15:17:40.559687Z","iopub.status.idle":"2025-04-05T15:17:40.574238Z","shell.execute_reply.started":"2025-04-05T15:17:40.559655Z","shell.execute_reply":"2025-04-05T15:17:40.573454Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/config.json","text/html":"👉 Download config.json: <a href='finetuned_model/config.json' target='_blank'>finetuned_model/config.json</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/generation_config.json","text/html":"👉 Download generation_config.json: <a href='finetuned_model/generation_config.json' target='_blank'>finetuned_model/generation_config.json</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/tokenizer.json","text/html":"👉 Download tokenizer.json: <a href='finetuned_model/tokenizer.json' target='_blank'>finetuned_model/tokenizer.json</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/pytorch_model.bin.index.json","text/html":"👉 Download pytorch_model.bin.index.json: <a href='finetuned_model/pytorch_model.bin.index.json' target='_blank'>finetuned_model/pytorch_model.bin.index.json</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/special_tokens_map.json","text/html":"👉 Download special_tokens_map.json: <a href='finetuned_model/special_tokens_map.json' target='_blank'>finetuned_model/special_tokens_map.json</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/pytorch_model-00002-of-00002.bin","text/html":"👉 Download pytorch_model-00002-of-00002.bin: <a href='finetuned_model/pytorch_model-00002-of-00002.bin' target='_blank'>finetuned_model/pytorch_model-00002-of-00002.bin</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/pytorch_model-00001-of-00002.bin","text/html":"👉 Download pytorch_model-00001-of-00002.bin: <a href='finetuned_model/pytorch_model-00001-of-00002.bin' target='_blank'>finetuned_model/pytorch_model-00001-of-00002.bin</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/finetuned_model/tokenizer_config.json","text/html":"👉 Download tokenizer_config.json: <a href='finetuned_model/tokenizer_config.json' target='_blank'>finetuned_model/tokenizer_config.json</a><br>"},"metadata":{}}],"execution_count":4}]}